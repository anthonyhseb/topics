{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dataset test 1\n",
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\"\n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc4 = \"Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.\"\n",
    "doc5 = \"Health experts say that Sugar is not good for your lifestyle.\"\n",
    "doc_test1=\"sugar is bad\"\n",
    "doc_test2=\"sugar is good\"\n",
    "#doc_complete = [doc1, doc2]\n",
    "doc_complete = [doc1, doc2, doc3, doc4, doc5]\n",
    "#doc_complete = [doc_test1, doc_test2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sugar is bad to consume. My sister likes to have sugar, but not my father.\n",
      " topiMy father spends a lot of time driving my sister around to dance practice.\n",
      " topiDoctors suggest that driving may cause increased stress and blood pressure.\n",
      " topiSometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.\n",
      " topiHealth experts say that Sugar is not good for your lifestyle.\n"
     ]
    }
   ],
   "source": [
    "print '\\n topi'.join(doc_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Basketball is a non-contact team sport played ...\n",
      "1    Football is a family of team sports that invol...\n",
      "2    Association football, more commonly known as f...\n",
      "3    American football, referred to as football in ...\n",
      "4    A football team is the collective name given t...\n",
      "5    Women's basketball is one of the few[citation ...\n",
      "6    The five basketball positions normally employe...\n",
      "7    The history of basketball is traced back to a ...\n",
      "Name: doc, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# dataset test 2\n",
    "import pandas as pd\n",
    "xl = pd.ExcelFile(\"data.xlsx\")\n",
    "df = xl.parse(\"data\")\n",
    "df.columns = ['sujet', 'doc']\n",
    "doc_complete = df['doc']\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "exclude = set(string.punctuation) \n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    # clean and tokenize document string\n",
    "    raw = doc.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in stop]\n",
    "    \n",
    "    # remove punc_free from tokens\n",
    "    punc_free = [ch for ch in stopped_tokens if ch not in exclude]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in punc_free]\n",
    "    \n",
    "    # lemm tokens\n",
    "    normalized = [lemma.lemmatize(word) for word in punc_free]\n",
    "   \n",
    "    # add tokens to list\n",
    "    return normalized\n",
    " \n",
    "doc_clean = [clean(doc) for doc in doc_complete] \n",
    "#print(doc_clean)\n",
    "#print(doc_clean1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ doc 0 =======================\n",
      "sister : 1\n",
      "consume : 1\n",
      "father : 1\n",
      "sugar : 2\n",
      "bad : 1\n",
      "like : 1\n",
      "============ doc 1 =======================\n",
      "sister : 1\n",
      "father : 1\n",
      "around : 1\n",
      "driving : 1\n",
      "dance : 1\n",
      "practice : 1\n",
      "lot : 1\n",
      "time : 1\n",
      "spends : 1\n",
      "============ doc 2 =======================\n",
      "driving : 1\n",
      "stress : 1\n",
      "increased : 1\n",
      "doctor : 1\n",
      "may : 1\n",
      "suggest : 1\n",
      "pressure : 1\n",
      "blood : 1\n",
      "cause : 1\n",
      "============ doc 3 =======================\n",
      "sister : 1\n",
      "father : 1\n",
      "pressure : 1\n",
      "school : 1\n",
      "never : 1\n",
      "perform : 1\n",
      "seems : 1\n",
      "sometimes : 1\n",
      "well : 1\n",
      "drive : 1\n",
      "better : 1\n",
      "feel : 1\n",
      "============ doc 4 =======================\n",
      "sugar : 1\n",
      "health : 1\n",
      "say : 1\n",
      "good : 1\n",
      "lifestyle : 1\n",
      "expert : 1\n"
     ]
    }
   ],
   "source": [
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "#print dic to ckeck\n",
    "#for k, v in dictionary.iteritems():\n",
    "    #print k, v\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "for i,doc in enumerate(doc_term_matrix):\n",
    "    print \"============ doc %s =======================\" %(i)\n",
    "    for d in doc:\n",
    "        print \"%s : %s\"% (dictionary[d[0]], d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1)]\n",
      "[(0, 1), (2, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)]\n",
      "[(7, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1)]\n",
      "[(0, 1), (2, 1), (18, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1)]\n",
      "[(3, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)]\n",
      "=================\n",
      "[(0, 1), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1)]\n"
     ]
    }
   ],
   "source": [
    "for i in doc_term_matrix:\n",
    "    print i\n",
    "print \"=================\"\n",
    "print doc_term_matrix[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel(num_terms=35, num_topics=5, decay=0.5, chunksize=2000)\n"
     ]
    }
   ],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "nb_topics=5\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=nb_topics, id2word = dictionary, passes=50)\n",
    "print(ldamodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.075*\"driving\" + 0.075*\"sister\" + 0.075*\"father\"'), (1, u'0.029*\"sugar\" + 0.029*\"driving\" + 0.029*\"sister\"'), (2, u'0.029*\"sugar\" + 0.029*\"driving\" + 0.029*\"sister\"'), (3, u'0.063*\"pressure\" + 0.063*\"sister\" + 0.063*\"father\"'), (4, u'0.092*\"sugar\" + 0.092*\"health\" + 0.092*\"say\"')]\n"
     ]
    }
   ],
   "source": [
    "# words by topic\n",
    "print(ldamodel.print_topics(num_topics=nb_topics, num_words=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.91982559354103033), (1, 0.020008397003264022), (2, 0.020008397003294244), (3, 0.020153248387034903), (4, 0.020004364065376649)]\n"
     ]
    }
   ],
   "source": [
    "#parametre : doc par term\n",
    "#resut: topic with score by doc id\n",
    "print(ldamodel.get_document_topics(doc_term_matrix[1]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
